"""История и развитие искусственного интеллекта (ИИ)
Введение
Искусственный интеллект (ИИ) — это область компьютерных наук, посвящённая созданию систем, способных выполнять задачи,
которые требуют человеческого интеллекта. Сюда входят такие задачи, как распознавание речи, принятие решений, перевод
текстов и распознавание изображений.
История развития ИИ
Ранние исследования (1950-е годы):
1950: Алан Тьюринг предложил тест Тьюринга, как критерий для определения интеллекта машины.
1956: В Дартмуте состоялась первая конференция по ИИ, которая считается началом формального развития этой области.
Золотой век ИИ (1960-1970-е годы):
В этот период были разработаны первые программы ИИ, такие как логические теоремы и системы, основанные на правилах.
1966: Проект ELIZA Джозефа Вейценбаума — первая программа, которая имитировала общение на естественном языке.
Зима ИИ (1970-1980-е годы):
Период упадка интереса и финансирования в области ИИ, вызванный разочарованием в результатах исследований и
ограничениями вычислительных мощностей.
Возрождение ИИ (1990-е годы):
Возрождение интереса к ИИ благодаря успехам в области машинного обучения и увеличению вычислительных мощностей.
1997: Компьютер Deep Blue от IBM победил чемпиона мира по шахматам Гарри Каспарова.
Современный ИИ (2000-е годы — настоящее время):
Бурное развитие глубокого обучения и нейронных сетей благодаря большим данным (Big Data) и мощным графическим
процессорам (GPU).
2012: Прорыв в распознавании изображений с использованием сверточных нейронных сетей (CNN) в конкурсе ImageNet.
Однослойный и многослойный перцептрон
Перцептрон — это простейшая форма искусственной нейронной сети, предложенная Фрэнком Розенблаттом в 1957 году.
Перцептрон используется для задач классификации.
Однослойный перцептрон
Однослойный перцептрон состоит из одного слоя нейронов и может решать только линейно разделимые задачи.
Пример кода однослойного перцептрона на Python"""


import numpy as np

# Функция активации (шаговая функция)
def step_function(x):
    return np.where(x >= 0, 1, 0)

class Perceptron:
    def __init__(self, input_size, learning_rate=0.01, epochs=1000):
        self.W = np.zeros(input_size + 1)
        self.learning_rate = learning_rate
        self.epochs = epochs

    def predict(self, x):
        return step_function(np.dot(self.W, x))

    def train(self, X, y):
        for _ in range(self.epochs):
            for xi, target in zip(X, y):
                xi = np.insert(xi, 0, 1)  # Вставка смещения (bias)
                prediction = self.predict(xi)
                self.W += self.learning_rate * (target - prediction) * xi

# Данные для обучения (И, ИЛИ)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])  # Операция И (AND)

perceptron = Perceptron(input_size=2)
perceptron.train(X, y)

# Тестирование
for xi in X:
    xi_with_bias = np.insert(xi, 0, 1)  # Вставка смещения (bias) для тестирования
    print(f"{xi} -> {perceptron.predict(xi_with_bias)}")

"""Многослойный перцептрон (MLP)
Многослойный перцептрон состоит из нескольких слоев нейронов и способен решать нелинейные задачи. 
Он включает входной слой, один или несколько скрытых слоев и выходной слой.
Пример кода многослойного перцептрона на Python с использованием библиотеки scikit-learn"""


from sklearn.neural_network import MLPClassifier
import numpy as np

# Данные для обучения (И, ИЛИ)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 0, 0, 1])  # Операция И (AND)

# Создание и обучение MLP-классификатора
mlp = MLPClassifier(hidden_layer_sizes=(5,), activation='relu', max_iter=400, learning_rate_init=0.01, solver='adam')
mlp.fit(X, y)

# Тестирование
for xi in X:
    print(f"{xi} -> {mlp.predict([xi])[0]}")


"""Задание
Многослойный перцептрон (MLP) может решать задачи, которые однослойный перцептрон не способен решить, 
из-за своей способности моделировать нелинейные зависимости. Одним из классических примеров такой задачи является 
проблема "исключающее ИЛИ" (XOR).
Проблема XOR
Логическая операция XOR возвращает истину только в том случае, если один из входов истинный, а другой ложный. 
Ниже приведена таблица истинности для XOR:
Вход 1	Вход 2	XOR
0	     0	     0
0	     1       1
1	     0 	     1
1	     1	     0
Эту задачу нельзя решить с помощью однослойного перцептрона, так как она не является линейно разделимой. 
Однако многослойный перцептрон, содержащий хотя бы один скрытый слой, способен решить эту задачу.
Ниже приведен код решения задачи XOR многослойным персептроном.
Разберите ее самостоятельно.
После чего создайте новую кодовую ячейку. Скопируйте в нее код однослойного персептрона и попытайтесь решить 
задачу на 10 000, 20 000, 50 000 эпохах.
Создайте текстовую ячейку и напишите в ней свои выводы об однослойных и многослойных персептронах.
Сохраните колаб и пришлите на него ссылку преподавателю."""

from sklearn.neural_network import MLPClassifier
import numpy as np

# Данные для обучения (XOR)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])  # Операция XOR

# Создание и обучение MLP-классификатора
mlp = MLPClassifier(hidden_layer_sizes=(5,), activation='relu', max_iter=5000, solver='adam')
mlp.fit(X, y)

# Тестирование
for xi in X:
    print(f"{xi} -> {mlp.predict([xi])[0]}")

"""МОЙ КОД"""


from sklearn.neural_network import MLPClassifier
import numpy as np

# Данные для обучения (XOR)
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = np.array([0, 1, 1, 0])  # Операция XOR

# Функция для обучения и тестирования персептрона
def train_and_test(epochs):
    # Создание и обучение MLP-классификатора с одним слоем (без скрытых нейронов)
    mlp = MLPClassifier(hidden_layer_sizes=(), activation='identity', max_iter=epochs, solver='adam')
    mlp.fit(X, y)

    # Тестирование
    print(f"Результаты для {epochs} эпох:")
    for xi in X:
        print(f"{xi} -> {mlp.predict([xi])[0]}")

# Тестирование на различных количествах эпох
train_and_test(10000)
train_and_test(20000)
train_and_test(50000)
