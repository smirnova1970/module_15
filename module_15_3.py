"""Основные концепции и терминология в области искусственного интеллекта (ИИ) охватывают широкий спектр тем,
связанных с созданием и использованием систем, которые могут выполнять задачи, требующие интеллекта.
Вот основные термины и концепции, которые вам нужно знать:
Основные концепции
Искусственный интеллект (ИИ):
Общий термин, обозначающий машины или системы, которые могут выполнять задачи, требующие человеческого интеллекта,
такие как понимание естественного языка, распознавание изображений, принятие решений и т. д.
Машинное обучение (ML):
Подмножество ИИ, в котором используются алгоритмы, позволяющие системам обучаться и улучшать свои функции на основе
данных. В машинном обучении выделяют несколько типов:
Обучение с учителем: Алгоритм обучается на размеченных данных, где каждому входу соответствует правильный выход.
Обучение без учителя: Алгоритм анализирует данные без предварительной разметки, выявляя скрытые структуры или
закономерности.
Обучение с подкреплением: Алгоритм обучается на основе обратной связи от среды, получая награды или штрафы
за свои действия.
Глубокое обучение (DL):
Подмножество машинного обучения, основанное на искусственных нейронных сетях с большим количеством слоев (глубокие
нейронные сети). Оно используется для задач, таких как обработка изображений, распознавание речи и понимание текста.
Обработка естественного языка (NLP):
Подобласть ИИ, занимающаяся взаимодействием между компьютерами и человеческими языками. Задачи включают понимание,
генерацию и перевод текста.
Основная терминология
Алгоритмы:
Последовательность инструкций, используемых для решения задач или выполнения вычислений.
Данные и датасеты:
Информация, на которой обучаются и тестируются алгоритмы ИИ. Датасеты могут быть размеченными (с метками) или
неразмеченными.
Нейронные сети:
Модели, вдохновленные биологическими нейронными сетями, состоящие из узлов (нейронов) и соединений (синапсов).
Включают слои: входной, скрытые и выходной.
Гиперпараметры:
Параметры модели, которые настраиваются до обучения и не изменяются в процессе обучения, такие как скорость обучения,
число слоев и число нейронов в каждом слое.
Обратное распространение (Backpropagation):
Алгоритм для обучения нейронных сетей, который использует градиентный спуск для минимизации ошибки, корректируя веса сети.
Градиентный спуск:
Оптимизационный метод, используемый для минимизации функции ошибки путем обновления параметров модели в направлении,
противоположном градиенту.
Регуляризация:
Техники, используемые для предотвращения переобучения модели, такие как L1 и L2 регуляризация, dropout и другие.
Переобучение и недообучение:
Переобучение (Overfitting): Модель хорошо обучается на тренировочных данных, но плохо обобщает на новых данных.
Недообучение (Underfitting): Модель не может хорошо обучиться даже на тренировочных данных.
Точность (Accuracy) и погрешность (Error):
Метрики, используемые для оценки производительности модели. Точность – доля правильно предсказанных случаев,
погрешность – разница между предсказанным и истинным значением.
Конволюционные нейронные сети (CNN):
Специализированные нейронные сети для обработки данных с сетчатой топологией, таких как изображения. Используют
сверточные слои для автоматического выделения признаков.
Рекуррентные нейронные сети (RNN):
Нейронные сети, предназначенные для работы с последовательными данными, такими как временные ряды и текст. Включают
механизмы, такие как LSTM и GRU, для хранения долгосрочных зависимостей.
Автокодировщики (Autoencoders): Нейронные сети, используемые для обучения эффективного кодирования данных.
Состоят из двух частей: кодировщика и декодировщика.
Генеративно-состязательные сети (GAN):
Модели, состоящие из двух нейронных сетей: генератора и дискриминатора, которые обучаются одновременно. Генератор
создает фальшивые данные, дискриминатор пытается отличить фальшивые данные от настоящих.
Трансформеры (Transformers): Архитектуры для обработки последовательных данных, особенно в NLP,
которые используют механизм внимания для параллельной обработки данных."""


"""Домашнее задание
Ознакомьтесь с приведёнными ниже примерами использования алгоритмов МО и НС для решения задачи распознавания рукописных цифр.
орядок выполнения ДЗ
Сделайте копию данного блокнота себе на диск. Далее работайте со своей копией блокнота. Сохраняйте вносимые в неё изменения.
Ознакомьтесь с теоретическим текстом и кодом из настоящего блокнота.
Перенесите примеры кода в отдельные кодовые ячейки и выполните их.
Создайте тестовую ячейку, куда запишите ответы на теоретические вопросы.
Расшарьте блокнот и используйте ссылку как ответ на ДЗ.
Учебный пример: Рещение задачи классификация рукописных цифр с использованием машинного обучения, глубокого обучения и нейронных сетей
В этом задании мы будем использовать набор данных MNIST, который содержит изображения рукописных цифр (от 0 до 9). Мы реализуем три различных подхода к классификации этих изображений:

Машинное обучение: Используем метод k-ближайших соседей (k-NN).
Глубокое обучение: Используем многослойный перцептрон (MLP).
Нейронные сети: Используем сверточную нейронную сеть (CNN).
Шаг 1: Установка библиотек
Установите необходимые библиотеки:

pip install numpy pandas scikit-learn tensorflow keras
Шаг 2: Загрузка и предобработка данных
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# Загрузка данных MNIST
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data / 255.0, mnist.target.astype(int)

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
Шаг 3: Алгоритм машинного обучения (k-NN)
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Создание и обучение модели k-NN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Прогнозирование на тестовой выборке
y_pred_knn = knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

print(f'Accuracy of k-NN: {accuracy_knn:.4f}')
Шаг 4: Глубокое обучение (MLP)
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Предобработка меток для MLP
y_train_mlp = to_categorical(y_train, 10)
y_test_mlp = to_categorical(y_test, 10)

# Создание модели MLP
model_mlp = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели MLP
model_mlp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_mlp.fit(X_train, y_train_mlp, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_mlp, accuracy_mlp = model_mlp.evaluate(X_test, y_test_mlp)

print(f'Accuracy of MLP: {accuracy_mlp:.4f}')
Шаг 5: Нейронные сети (CNN)
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout

# Предобработка данных для CNN
X_train_cnn = X_train.values.reshape(-1, 28, 28, 1)
X_test_cnn = X_test.values.reshape(-1, 28, 28, 1)
y_train_cnn = to_categorical(y_train, 10)
y_test_cnn = to_categorical(y_test, 10)

# Создание модели CNN
model_cnn = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели CNN
model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test_cnn, y_test_cnn)

print(f'Accuracy of CNN: {accuracy_cnn:.4f}')
Заключение
В этом задании мы реализовали три различных подхода к классификации изображений рукописных цифр с использованием средств машинного обучения (k-NN), глубокого обучения (MLP) и нейронных сетей (CNN). Мы увидели, что каждый из этих подходов имеет свои преимущества и недостатки, и что сложные модели глубокого обучения могут значительно улучшить точность классификации по сравнению с простыми моделями машинного обучения.

Теоритические вопросы
Какие преимущества и недостатки использованных методов вы увидели?
В чем, на ваш взгляд, заключается принципиальная разница между многослойным перцептроном и сверточной нейронной сетью?
Какие методы предобработки данных были использованы в этом задании?"""


pip install numpy pandas scikit-learn tensorflow keras

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.datasets import fetch_openml
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.utils import to_categorical

# Загрузка данных MNIST
mnist = fetch_openml('mnist_784', version=1)
X, y = mnist.data / 255.0, mnist.target.astype(int)

# Разделение данных на обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# Создание и обучение модели k-NN
knn = KNeighborsClassifier(n_neighbors=3)
knn.fit(X_train, y_train)

# Прогнозирование на тестовой выборке
y_pred_knn = knn.predict(X_test)
accuracy_knn = accuracy_score(y_test, y_pred_knn)

print(f'Accuracy of k-NN: {accuracy_knn:.4f}')

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten

# Предобработка меток для MLP
y_train_mlp = to_categorical(y_train, 10)
y_test_mlp = to_categorical(y_test, 10)

# Создание модели MLP
model_mlp = Sequential([
    Flatten(input_shape=(784,)),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели MLP
model_mlp.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_mlp.fit(X_train, y_train_mlp, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_mlp, accuracy_mlp = model_mlp.evaluate(X_test, y_test_mlp)

print(f'Accuracy of MLP: {accuracy_mlp:.4f}')


from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout

# Предобработка данных для CNN
X_train_cnn = X_train.values.reshape(-1, 28, 28, 1)
X_test_cnn = X_test.values.reshape(-1, 28, 28, 1)
y_train_cnn = to_categorical(y_train, 10)
y_test_cnn = to_categorical(y_test, 10)

# Создание модели CNN
model_cnn = Sequential([
    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPooling2D(pool_size=(2, 2)),
    Dropout(0.25),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(10, activation='softmax')
])

# Компиляция и обучение модели CNN
model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model_cnn.fit(X_train_cnn, y_train_cnn, epochs=10, batch_size=32, validation_split=0.2)

# Оценка модели на тестовой выборке
loss_cnn, accuracy_cnn = model_cnn.evaluate(X_test_cnn, y_test_cnn)

print(f'Accuracy of CNN: {accuracy_cnn:.4f}')


"""Теоритические вопросы
1. Какие преимущества и недостатки использованных методов вы увидели?
2. В чем,на ваш взгляд, заключается принципиальная разница между многослойным перцептроном и сверточной нейронной сетью?
3. Какие методы предобработки данных были использованы в этом задании?
                        ОТВЕТ
1. Преимущества и недостатки использованных методов
                     Преимущества:
Метод k-ближайших соседей (k-NN): Простота и легкость в реализации. Не требует предварительного обучения модели, что 
позволяет быстро проводить тестирование. Может быть эффективным на небольших наборах данных.
Многослойный перцептрон (MLP): Способен моделировать сложные нелинейные зависимости. После обучения предсказания 
выполняются быстро, так как не требуется анализировать всю обучающую выборку. Гибкость в применении для различных задач 
классификации и регрессии.
Сверточная нейронная сеть (CNN): Эффективно извлекает пространственные признаки из изображений, что значительно улучшает 
качество классификации. Автоматически определяет важные признаки, что снижает необходимость в ручной предобработке. 
Способна обрабатывать изображения с различными размерами и ориентацией.

                        Недостатки:
Метод k-ближайших соседей (k-NN): Высокая вычислительная сложность при предсказании, особенно на больших наборах данных, 
так как нужно сравнивать новое значение с каждым элементом обучающей выборки. Чувствителен к шуму и выбросам в данных. 
Неэффективен при высокой размерности данных (проклятие размерности).
Многослойный перцептрон (MLP): Требует значительного объема данных для достижения хороших результатов и предотвращения 
переобучения. Не оптимизирован для обработки изображений, так как не учитывает пространственные связи между пикселями.
Сверточная нейронная сеть (CNN): Сложность архитектуры и процесса обучения, требующая больше времени и вычислительных 
ресурсов. Зависит от подбора гиперпараметров и архитектуры, что может требовать значительных усилий для оптимизации. 
Может быть подвержена переобучению при недостаточном количестве данных.

2. Принципиальная разница между многослойным перцептроном и сверточной нейронной сетью.
Основная разница между многослойным перцептроном (MLP) и сверточной нейронной сетью (CNN) заключается в способе 
обработки данных:
MLP:
Состоит из полносвязных слоев, где каждый нейрон связан со всеми нейронами предыдущего слоя. Не учитывает 
пространственные зависимости, что делает его менее эффективным для обработки изображений. Для работы с изображениями 
требуется предварительное преобразование в векторный формат, что может привести к потере информации о пространственной 
структуре. CNN:
Использует свертки, которые сосредоточены на локальных участках изображения и позволяют модели выявлять 
пространственные зависимости. Содержит слои подвыборки (пулинга), которые уменьшают размерность данных и позволяют 
сосредоточиться на наиболее значимых признаках. Имеет меньшую сложность в плане параметров по сравнению с MLP, 
что позволяет лучше обобщать на новых данных.

3. Методы предобработки данных В этом задании были использованы следующие методы предобработки данных:
Нормализация:
Данные изображения были нормализованы путем деления на 255, чтобы привести значения пикселей в диапазон от 0 до 1. 
Это улучшает сходимость алгоритмов обучения. Разделение выборки:
Данные были разделены на обучающую и тестовую выборки с использованием функции train_test_split, что позволяет оценить 
производительность модели на невидимых данных.
Преобразование меток:
Метки классов для MLP были преобразованы в формат one-hot с использованием функции to_categorical, что позволяет 
модели корректно интерпретировать многоклассовую задачу.
Изменение формы данных для CNN:
Данные были преобразованы в трехмерный формат (28, 28, 1), что соответствует формату изображений с одним цветовым 
каналом (градации серого), позволяя CNN обрабатывать данные должным образом. GPT-4o mini"""
